import os

# ë¶€ë™ì‚°ë±…í¬ ì ‘ì†id(wfight66/ëª½ì…ì´69)
# const
# regions = [
#     {name: "ì„œìš¸íŠ¹ë³„ì‹œ", url: "https://www.neonet.co.kr/novo-rebank/view/agency_zone/AgencyZoneIndex.neo?lcode=01"},
#     {name: "ê²½ê¸°ë„", url: "https://www.neonet.co.kr/novo-rebank/view/agency_zone/AgencyZoneIndex.neo?lcode=02"},
#     {name: "ë¶€ì‚°ê´‘ì—­ì‹œ", url: "https://www.neonet.co.kr/novo-rebank/view/agency_zone/AgencyZoneIndex.neo?lcode=03"},
#     {name: "ëŒ€êµ¬ê´‘ì—­ì‹œ", url: "https://www.neonet.co.kr/novo-rebank/view/agency_zone/AgencyZoneIndex.neo?lcode=04"},
#     {name: "ì¸ì²œê´‘ì—­ì‹œ", url: "https://www.neonet.co.kr/novo-rebank/view/agency_zone/AgencyZoneIndex.neo?lcode=05"},
#     {name: "ê´‘ì£¼ê´‘ì—­ì‹œ", url: "https://www.neonet.co.kr/novo-rebank/view/agency_zone/AgencyZoneIndex.neo?lcode=06"},
#     {name: "ëŒ€ì „ê´‘ì—­ì‹œ", url: "https://www.neonet.co.kr/novo-rebank/view/agency_zone/AgencyZoneIndex.neo?lcode=07"},
#     {name: "ê°•ì›ë„", url: "https://www.neonet.co.kr/novo-rebank/view/agency_zone/AgencyZoneIndex.neo?lcode=08"},
#     {name: "ê²½ìƒë‚¨ë„", url: "https://www.neonet.co.kr/novo-rebank/view/agency_zone/AgencyZoneIndex.neo?lcode=10"},
#     {name: "ê²½ìƒë¶ë„", url: "https://www.neonet.co.kr/novo-rebank/view/agency_zone/AgencyZoneIndex.neo?lcode=11"},
#     {name: "ì „ë¼ë‚¨ë„", url: "https://www.neonet.co.kr/novo-rebank/view/agency_zone/AgencyZoneIndex.neo?lcode=12"},
#     {name: "ì „ë¼ë¶ë„", url: "https://www.neonet.co.kr/novo-rebank/view/agency_zone/AgencyZoneIndex.neo?lcode=13"},
#     {name: "ì œì£¼íŠ¹ë³„ìì¹˜ë„", url: "https://www.neonet.co.kr/novo-rebank/view/agency_zone/AgencyZoneIndex.neo?lcode=14"},
#     {name: "ì¶©ì²­ë‚¨ë„", url: "https://www.neonet.co.kr/novo-rebank/view/agency_zone/AgencyZoneIndex.neo?lcode=15"},
#     {name: "ì¶©ì²­ë¶ë„", url: "https://www.neonet.co.kr/novo-rebank/view/agency_zone/AgencyZoneIndex.neo?lcode=16"},
#     {name: "ìš¸ì‚°ê´‘ì—­ì‹œ", url: "https://www.neonet.co.kr/novo-rebank/view/agency_zone/AgencyZoneIndex.neo?lcode=17"},
#     {name: "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ", url: "https://www.neonet.co.kr/novo-rebank/view/agency_zone/AgencyZoneIndex.neo?lcode=19"},
# ];
# ì„œìš¸/ê°•ë‚¨êµ¬/ê°œí¬ë™ => lcode:ì§€ì—­, mcode: ì‹œêµ°êµ¬, sname: ë™/ì
# https://www.neonet.co.kr/novo-rebank/view/market_price/PastMarketPriceList.neo?lcode=01&mcode=135&sname=ê°œí¬ë™

# ì‹œêµ°êµ¬ì½”ë“œ ê°€ì ¸ì˜¤ê¸°
# https://www.neonet.co.kr/novo-rebank/view/market_price/RegionData.neo?offerings_gbn=AT&lcode=01&mcode=&target=mcode&update=140228
# # ê°•ë‚¨êµ¬(135)ì— ìëª…ë™ëª… ê°€ì ¸ì˜¤ê¸°
# https://www.neonet.co.kr/novo-rebank/view/market_price/RegionData.neo?offerings_gbn=AT&lcode=01&mcode=135&target=sname&update=140228
# # ìƒì„¸í˜„í™©ëª©ë¡ - ë…¼í˜„ë™
# https://www.neonet.co.kr/novo-rebank/view/market_price/PastMarketPriceList.neo?lcode=01&mcode=135&sname=%B3%ED%C7%F6%B5%BF
#
# if (document.getElementById('sname') && document.getElementById('sname').value) {
#     param += '&sname=' + encodeURIComponent(document.getElementById('sname').value);
# }
# https://www.neonet.co.kr/novo-rebank/view/market_price/PastMarketPriceList.neo?lcode=01&mcode=135&sname=encodeURIComponent('ì‚¼ì„±ë™')
# ì—°ê°„ìë£Œ ì¶œë ¥ ìƒì„¸í˜„í™©(ì œì£¼ë„=>ì œì£¼ì‹œ=>í™”ë¶ë™=>í™”ë¶ì£¼ê³µ4ë‹¨ì§€
# https://www.neonet.co.kr/novo-rebank/view/market_price/PastMarketPriceDetail.neo?lcode=14&mcode=690&sname=%C8%AD%BA%CF%B5%BF&complex_cd=A1012009&pyung_cd=1
#
import requests
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait, Select
from selenium.webdriver.support import expected_conditions as EC
from datetime import datetime
import time
import re
import json
import csv
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
from urllib.parse import quote

from pastapt.past_apt_db_utils import past_apt_create_table, insert_apt_and_prices

# í¼ë¡œê·¸ì¸ì²˜ë¦¬
def login(driver):
    try:
        # ì•„ì´ë”” ì…ë ¥ í•„ë“œ ë¡œë”© ëŒ€ê¸°
        username_field = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.NAME, "id"))
        )
        password_field = driver.find_element(By.NAME, "pw")

        # ë¡œê·¸ì¸ ì •ë³´ ì…ë ¥
        username_field.clear()
        username_field.send_keys("wfight69")
        password_field.send_keys("ahdtpddl69")

        # ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­
        submit_button = driver.find_element(By.XPATH, "//input[@type='image' and contains(@src, 'btn_login01.gif')]")
        submit_button.click()

        print("ë¡œê·¸ì¸ ì‹œë„ ì™„ë£Œ.")
    except Exception as e:
        print("ë¡œê·¸ì¸ ì˜¤ë¥˜:", e)

# ì§€ì—­ ì •ë³´ ë¦¬ìŠ¤íŠ¸
regions = [
    {"name": "ì„œìš¸íŠ¹ë³„ì‹œ", "lcode": "01"},
    {"name": "ê²½ê¸°ë„", "lcode": "02"},
    {"name": "ë¶€ì‚°ê´‘ì—­ì‹œ", "lcode": "03"},
    {"name": "ëŒ€êµ¬ê´‘ì—­ì‹œ", "lcode": "04"},
    {"name": "ì¸ì²œê´‘ì—­ì‹œ", "lcode": "05"},
    {"name": "ê´‘ì£¼ê´‘ì—­ì‹œ", "lcode": "06"},
    {"name": "ëŒ€ì „ê´‘ì—­ì‹œ", "lcode": "07"},
    {"name": "ê°•ì›ë„", "lcode": "08"},
    {"name": "ê²½ìƒë‚¨ë„", "lcode": "10"},
    {"name": "ê²½ìƒë¶ë„", "lcode": "11"},
    {"name": "ì „ë¼ë‚¨ë„", "lcode": "12"},
    {"name": "ì „ë¼ë¶ë„", "lcode": "13"},
    {"name": "ì œì£¼íŠ¹ë³„ìì¹˜ë„", "lcode": "14"},
    {"name": "ì¶©ì²­ë‚¨ë„", "lcode": "15"},
    {"name": "ì¶©ì²­ë¶ë„", "lcode": "16"},
    {"name": "ìš¸ì‚°ê´‘ì—­ì‹œ", "lcode": "17"},
    {"name": "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ", "lcode": "19"}
]

# ì‹œêµ°êµ¬ê°€ì ¸ì˜¤ê¸° ì½”ë“œ
def get_mcode_list(lcode):
    url = f"https://www.neonet.co.kr/novo-rebank/view/market_price/RegionData.neo?offerings_gbn=AT&lcode={lcode}&mcode=&target=mcode&update=140228"
    headers = {
        "User-Agent": "Mozilla/5.0"
    }
    try:
        response = requests.get(url, headers=headers)
        response.encoding = "euc-kr"  # ì‘ë‹µ ì¸ì½”ë”© ì„¤ì •
        if response.status_code != 200:
            print(f"ìš”ì²­ ì‹¤íŒ¨: ìƒíƒœ ì½”ë“œ {response.status_code}")
            return []

        root = ET.fromstring(response.text)
        mcode_list = []

        for node in root.findall(".//n"):
            mcode = node.find("code").text.strip()
            name = node.find("name").text.strip()
            mcode_list.append({"mcode": mcode, "name": name})

        return mcode_list

    except Exception as e:
        print("ì‹œêµ°êµ¬ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨:", e)
        return []

# ìœ„ ì‹œ.êµ°.êµ¬ì½”ë“œ(mcode)ë¥¼ ìê¸°ê³ ì„œ ì.ë©´.ë™ì„ ê°€ì ¸ì˜¤ê¸°
def get_sname_list(lcode, mcode):
    url = f"https://www.neonet.co.kr/novo-rebank/view/market_price/RegionData.neo?offerings_gbn=AT&lcode={lcode}&mcode={mcode}&target=sname&update=140228"
    headers = {
        "User-Agent": "Mozilla/5.0"
    }
    try:
        response = requests.get(url, headers=headers)
        response.encoding = "euc-kr"
        if response.status_code != 200:
            print(f"ìš”ì²­ ì‹¤íŒ¨: ìƒíƒœ ì½”ë“œ {response.status_code}")
            return []

        root = ET.fromstring(response.text)
        sname_list = []

        for node in root.findall(".//n"):
            code = node.find("code").text.strip()
            name = node.find("name").text.strip()
            sname_list.append({"sname_code": code, "sname": name})

        return sname_list

    except Exception as e:
        print("ìë©´ë™ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨:", e)
        return []


# ì‹œêµ°êµ¬=>ìë©´ë™=>ì•„íŒŒíŠ¸ëª©ë¡
def get_past_market_list(lcode, mcode, sname):
    encoded_sname = quote(sname, encoding="euc-kr")
    url = f"https://www.neonet.co.kr/novo-rebank/view/market_price/PastMarketPriceList.neo?lcode={lcode}&mcode={mcode}&sname={encoded_sname}"

    headers = {
        "User-Agent": "Mozilla/5.0"
    }
    try:
        response = requests.get(url, headers=headers)
        response.encoding = "euc-kr"

        if response.status_code != 200:
            print(f"[ìš”ì²­ ì‹¤íŒ¨] ìƒíƒœì½”ë“œ: {response.status_code}")
            return None

        soup = BeautifulSoup(response.text, "html.parser")
        container = soup.find("div", id="divMarketPriceList")
        if not container:
            print("ì‹œì„¸ í…Œì´ë¸” divë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        rows = container.find_all("tr")
        result = []
        current_apt_name = None
        current_apt_link = None

        for row in rows:
            tds = row.find_all("td")
            if not tds:
                continue

            # ì•„íŒŒíŠ¸ëª… ìˆëŠ” í–‰ (rowspan ìˆëŠ” ì²« ì¤„)
            if tds[0].has_attr("rowspan"):
                current_apt_name = tds[0].get_text(strip=True)
                link_tag = tds[0].find("a")
                current_apt_link = link_tag["href"] if link_tag else None

                size = tds[1].get_text(strip=True)
                detail_link = tds[1].find("a")["href"] if tds[1].find("a") else None
                prices = [td.get_text(strip=True) for td in tds[2:]]

            # ë‘ ë²ˆì§¸ ì¤„ (ì•„íŒŒíŠ¸ëª… ìƒëµëœ ì¤„)
            else:
                size = tds[0].get_text(strip=True)
                detail_link = tds[0].find("a")["href"] if tds[0].find("a") else None
                prices = [td.get_text(strip=True) for td in tds[1:]]

            result.append({
                "apartment": current_apt_name,
                "size": size,
                "price_history": prices,
                "apt_link": current_apt_link,
                "detail_link": detail_link
            })

        return result

    except Exception as e:
        print(f"[ì—ëŸ¬] {e}")
        return None

# ìƒì„¸ë…„ë„ë³„ ë°ì´íƒ€ íŒŒì‹±ì²˜ë¦¬ = ë™ì‘ì•ˆí•¨
def parse_detail_market_price(detail_url):
    full_url = f"{detail_url}"
    headers = {
        "User-Agent": "Mozilla/5.0"
    }
    try:
        response = requests.get(full_url, headers=headers)
        response.encoding = "euc-kr"

        if response.status_code != 200:
            print(f"[ìš”ì²­ ì‹¤íŒ¨] ìƒíƒœ ì½”ë“œ: {response.status_code}")
            return None

        soup = BeautifulSoup(response.text, "html.parser")
        container = soup.find("div", id="divMarketPriceList")
        if not container:
            print("ìƒì„¸ ì‹œì„¸ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        table = container.find("table")
        rows = table.find_all("tr")[2:]  # í—¤ë” ë‘ ì¤„ ì œì™¸

        detail_data = []

        for row in rows:
            cols = [td.get_text(strip=True) for td in row.find_all("td")]
            if len(cols) == 7:
                detail_data.append({
                    "month": cols[0],
                    "sale_low": cols[1],
                    "sale_high": cols[2],
                    "sale_change": cols[3],
                    "rent_low": cols[4],
                    "rent_high": cols[5],
                    "rent_change": cols[6]
                })

        return detail_data

    except Exception as e:
        print(f"[ìƒì„¸ í˜ì´ì§€ íŒŒì‹± ì˜¤ë¥˜] {e}")
        return None

def parse_value(val):
    return 0 if val in ["-", "--"] else val
#
def parse_detail_market_price_selenium(driver):

    try:
        WebDriverWait(driver, 1).until(
            EC.presence_of_element_located((By.ID, "divMarketPriceList"))
        )
        time.sleep(0.3)  # ì•½ê°„ì˜ ì•ˆì •ì„± ëŒ€ê¸°
        soup = BeautifulSoup(driver.page_source, "html.parser")

        # ìƒì„¸ ì‹œì„¸ ì •ë³´ íŒŒì‹±
        detail_data = []
        container = soup.find("div", id="divMarketPriceList")
        if not container:
            print("ìƒì„¸ ì‹œì„¸ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            table = container.find("table")
            rows = table.find_all("tr")[2:]  # í—¤ë” ë‘ ì¤„ ì œì™¸

            for row in rows:
                cols = [td.get_text(strip=True) for td in row.find_all("td")]
                if len(cols) == 7:
                    detail_data.append({
                        "month": cols[0],
                        "sale_low": parse_value(cols[1]),
                        "sale_high": parse_value(cols[2]),
                        "sale_change": cols[3],
                        "rent_low": parse_value(cols[4]),
                        "rent_high": parse_value(cols[5]),
                        "rent_change": cols[6]
                    })

        # ë‹¨ì§€ í”„ë¡œí•„ ì •ë³´ íŒŒì‹±
        profile = {}
        profile_container = soup.find("div", class_="reg_list_all")
        if profile_container:
            inner_table = profile_container.find("table")
            if inner_table:
                rows = inner_table.find_all("tr")
                for row in rows:
                    cols = row.find_all("td")
                    for col in cols:
                        text = col.get_text(strip=True)
                        if "ì´ ê°€êµ¬ìˆ˜" in text:
                            profile['total_households'] = text.split(":")[-1].strip()
                        elif "ì…ì£¼ë…„ë„" in text:
                            profile['move_in_date'] = text.split(":")[-1].strip()
                        elif "ê±´ì„¤ì‚¬" in text:
                            profile['builder'] = text.split(":")[-1].strip()
                        elif "ë™ìˆ˜/ì¸µìˆ˜" in text:
                            profile['building_floors'] = text.split(":")[-1].strip()
                        elif "ì£¼ì°¨ëŒ€ìˆ˜" in text:
                            profile['parking'] = text.split(":")[-1].strip()
                        elif "ë‚œë°©ë°©ì‹" in text:
                            profile['heating_type'] = text.split(":")[-1].strip()
        else:
            print("ë‹¨ì§€ í”„ë¡œí•„ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        return {
            "profile": profile,
            "market_prices": detail_data
        }

    except Exception as e:
        print(f"[í˜ì´ì§€ íŒŒì‹± ì˜¤ë¥˜] {e}")
        return None


# ë°˜ë³µ ë¶„ì„ìš© í•¨ìˆ˜ ì˜ˆì‹œ
def analyze_from_json(driver):
    try:
        # database & table ìƒì„±
        past_apt_create_table()

        filename = "region_hierarchy.json"
        with open(filename, encoding="utf-8-sig") as f:
            hierarchy = json.load(f)

        # JSON ê³„ì¸µêµ¬ì¡° ìˆœíšŒ: ëŒ€ë¶„ë¥˜(ì§€ì—­) â†’ ì¤‘ë¶„ë¥˜(ì‹œêµ°êµ¬) â†’ ì†Œë¶„ë¥˜(ìë©´ë™)
        for region in hierarchy:
            region_name = region["region_name"]
            lcode = region["lcode"]
            for mcode_obj in region.get("í•˜ìœ„", []):
                mcode = mcode_obj["mcode"]
                mcode_name = mcode_obj["mcode_name"]
                for s_obj in mcode_obj.get("í•˜ìœ„", []):
                    sname = s_obj["sname"]
                    print(f"\n[ {region_name} > {mcode_name} > {sname} ]")
                    data = get_past_market_list(lcode, mcode, sname)
                    if data:
                        for row in data:
                            time.sleep(0.1)  # ì•½ê°„ì˜ ì•ˆì •ì„± ëŒ€ê¸°
                            apt_name = row["apartment"]
                            size = re.sub(r'[^0-9.]', '', row["size"])
                            print(f"{apt_name} | {size}ã¡ | ê°€ê²©: {row['price_history']}")
                            print(f"ìƒì„¸ ë§í¬: {row['detail_link']}")
                            print()
                            if row['detail_link']:
                                detail_url = row['detail_link']
                                driver.get(detail_url)  # driverëŠ” ì—¬ê¸°ì„œ í•œ ë²ˆë§Œ ì´ë™
                                # detail = parse_detail_market_price(detail_url)
                                detail = parse_detail_market_price_selenium(driver)
                                if detail:
                                    profile = detail.get("profile", {})
                                    market_prices = detail.get("market_prices", [])
                                    print(f"ğŸ“Œ ë‹¨ì§€ í”„ë¡œí•„ ì •ë³´({apt_name})")
                                    print(f" - í‰í˜•(ê³µê¸‰)       : {size}m2")
                                    print(f" - ì´ ê°€êµ¬ìˆ˜        : {profile.get('total_households', 'ì •ë³´ ì—†ìŒ')}")
                                    print(f" - ì…ì£¼ë…„ë„         : {profile.get('move_in_date', 'ì •ë³´ ì—†ìŒ')}")
                                    print(f" - ê±´ì„¤ì‚¬          : {profile.get('builder', 'ì •ë³´ ì—†ìŒ')}")
                                    print(f" - ë™ìˆ˜/ì¸µìˆ˜        : {profile.get('building_floors', 'ì •ë³´ ì—†ìŒ')}")
                                    print(f" - ì£¼ì°¨ëŒ€ìˆ˜         : {profile.get('parking', 'ì •ë³´ ì—†ìŒ')}")
                                    print(f" - ë‚œë°©ë°©ì‹         : {profile.get('heating_type', 'ì •ë³´ ì—†ìŒ')}")
                                    print("============================")
                                    print("\nğŸ“Š ì›”ë³„ ì‹œì„¸ ì •ë³´")
                                    for d in market_prices:
                                        print(
                                            f" - {d['month']} | ë§¤ë§¤ {d['sale_low']}~{d['sale_high']} ({d['sale_change']}), ì „ì„¸ {d['rent_low']}~{d['rent_high']} ({d['rent_change']})"
                                        )
                                    print("\n")
                                    #
                                    insert_apt_and_prices(apt_name, detail, region_name, mcode_name, sname, size)

    except FileNotFoundError:
        print(f"âš ï¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")


# ì§€ì—­ì½”ë“œ => ì‹œêµ°êµ¬ëª… => ìë©´ë™ëª…ì„ ì €ì¥í•¨.
def save_region_hierarchy_to_json(filename="region_hierarchy.json"):
    # íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ íŒ¨ìŠ¤
    if os.path.exists(filename):
        print(f"âœ… íŒŒì¼ ì´ë¯¸ ì¡´ì¬: {filename} â†’ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    hierarchy = []  # ìµœì¢… JSON ë°ì´í„°ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸

    for region in regions:
        region_name = region["name"]
        lcode = region["lcode"]
        # ëŒ€ë¶„ë¥˜: ì§€ì—­ ì½”ë“œ
        region_obj = {
            "lcode": lcode,
            "region_name": region_name,
            "í•˜ìœ„": []  # ì‹œêµ°êµ¬ ëª©ë¡
        }

        mcode_list = get_mcode_list(lcode)
        for m in mcode_list:
            mcode = m["mcode"]
            mcode_name = m["name"]
            # ì¤‘ë¶„ë¥˜: ì‹œêµ°êµ¬
            mcode_obj = {
                "mcode": mcode,
                "mcode_name": mcode_name,
                "í•˜ìœ„": []  # ìë©´ë™ ëª©ë¡
            }

            sname_list = get_sname_list(lcode, mcode)
            for item in sname_list:
                sname = item["sname"]
                sname_code = item["sname_code"]
                # ì†Œë¶„ë¥˜: ìë©´ë™
                s_obj = {
                    "sname_code": sname_code,
                    "sname": sname
                }
                mcode_obj["í•˜ìœ„"].append(s_obj)

            region_obj["í•˜ìœ„"].append(mcode_obj)

        hierarchy.append(region_obj)

    # JSON íŒŒì¼ë¡œ ì €ì¥ (í•œê¸€ì´ ê¹¨ì§€ì§€ ì•Šë„ë¡ ensure_ascii=False)
    with open(filename, "w", encoding="utf-8-sig") as f:
        json.dump(hierarchy, f, ensure_ascii=False, indent=2)

    print(f"âœ… ì €ì¥ ì™„ë£Œ: {filename}")


def main():
    global json_data, saved_count, data_list  # ì „ì—­ ë³€ìˆ˜ ì‚¬ìš©

    # í¬ë¡¬ë“œë¼ì´ë²„ í™”ë©´ì—†ì´ ë™ì‘í•˜ê²Œ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•(ë°°ì¹˜ê°œë…ì— ì ìš©)
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    # í•„ìš”ì— ë”°ë¼ ì¶”ê°€ ì˜µì…˜ ì„¤ì •: --no-sandbox, --disable-dev-shm-usage ë“±

    driver = webdriver.Chrome(options=chrome_options)
    #driver = webdriver.Chrome()
    try:
        driver.get("https://www.neonet.co.kr/novo-rebank/view/member/MemberLogin.neo")
        driver.implicitly_wait(1)
        #=====
        login(driver)
        time.sleep(2)

        # ì§€ì—­ë¶„ì„ìœ„í•œ ì½”ë“œ í™”ì¼ì„ ë¨¼ì € ì €ì¥
        save_region_hierarchy_to_json()

        # ë¶„ì„ì‹œì‘ì²˜ë¦¬
        analyze_from_json(driver)

    except Exception as e:
        print("ì˜¤ë¥˜ ë°œìƒ:", e)
    finally:
        driver.quit()

if __name__ == "__main__":
    main()